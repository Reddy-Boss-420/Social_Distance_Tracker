{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc7169d-ac8f-4e86-88b2-3e87917fdf65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jetson.inference\n",
    "import jetson.utils\n",
    "import ipywidgets \n",
    "from IPython.display import display\n",
    "from jetcam.utils import bgr8_to_jpeg as col\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c5e56fe-e528-4ba5-8949-8b8be454b071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_close(p1, p2, p3):\n",
    "    dst = math.sqrt(p1 ** 2 + p2 ** 2 + p3**2)\n",
    "    return dst \n",
    "def convertBack(x, y, w, h): \n",
    "    #================================================================\n",
    "    # 2.Purpose : Converts center coordinates to rectangle coordinates\n",
    "    #================================================================  \n",
    "    \"\"\"\n",
    "    :param:\n",
    "    x, y = midpoint of bbox\n",
    "    w, h = width, height of the bbox\n",
    "    \n",
    "    :return:\n",
    "    xmin, ymin, xmax, ymax\n",
    "    \"\"\"\n",
    "    xmin = int(round(x - (w / 2)))\n",
    "    xmax = int(round(x + (w / 2)))\n",
    "    ymin = int(round(y - (h / 2)))\n",
    "    ymax = int(round(y + (h / 2)))\n",
    "    return xmin, ymin, xmax, ymax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460a2210-b8f2-4a0b-8b77-eb8e633c569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import depthai as dai\n",
    "\n",
    "stepSize = 0.05\n",
    "\n",
    "newConfig = False\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define sources and outputs\n",
    "monoLeft = pipeline.createMonoCamera()\n",
    "monoRight = pipeline.createMonoCamera()\n",
    "stereo = pipeline.createStereoDepth()\n",
    "spatialLocationCalculator = pipeline.createSpatialLocationCalculator()\n",
    "camRgb = pipeline.createColorCamera()\n",
    "xoutRgb = pipeline.createXLinkOut()\n",
    "camRgb.preview.link(xoutRgb.input)\n",
    "\n",
    "xoutDepth = pipeline.createXLinkOut()\n",
    "xoutSpatialData = pipeline.createXLinkOut()\n",
    "xinSpatialCalcConfig = pipeline.createXLinkIn()\n",
    "\n",
    "xoutDepth.setStreamName(\"depth\")\n",
    "xoutSpatialData.setStreamName(\"spatialData\")\n",
    "xinSpatialCalcConfig.setStreamName(\"spatialCalcConfig\")\n",
    "xoutRgb.setStreamName(\"rgb\")\n",
    "\n",
    "# Properties\n",
    "monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_720_P)\n",
    "monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_720_P)\n",
    "monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "camRgb.setPreviewSize(1280, 720)\n",
    "camRgb.setInterleaved(False)\n",
    "camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.RGB)\n",
    "\n",
    "lrcheck = False\n",
    "subpixel = False\n",
    "\n",
    "stereo.initialConfig.setConfidenceThreshold(255)\n",
    "stereo.setLeftRightCheck(lrcheck)\n",
    "stereo.setSubpixel(subpixel)\n",
    "# Config\n",
    "topLeft = dai.Point2f(0.4, 0.4)\n",
    "bottomRight = dai.Point2f(0.6, 0.6)\n",
    "\n",
    "config = dai.SpatialLocationCalculatorConfigData()\n",
    "config.depthThresholds.lowerThreshold = 100\n",
    "config.depthThresholds.upperThreshold = 10000\n",
    "config.roi = dai.Rect(topLeft, bottomRight)\n",
    "\n",
    "spatialLocationCalculator.setWaitForConfigInput(False)\n",
    "spatialLocationCalculator.initialConfig.addROI(config)\n",
    "\n",
    "# Linking\n",
    "monoLeft.out.link(stereo.left)\n",
    "monoRight.out.link(stereo.right)\n",
    "\n",
    "spatialLocationCalculator.passthroughDepth.link(xoutDepth.input)\n",
    "stereo.depth.link(spatialLocationCalculator.inputDepth)\n",
    "\n",
    "spatialLocationCalculator.out.link(xoutSpatialData.input)\n",
    "xinSpatialCalcConfig.out.link(spatialLocationCalculator.inputConfig)\n",
    "\n",
    "device = dai.Device(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa4d0452-8cd9-4c69-84f6-d107b072e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(centroid_dict, n):\n",
    "    \n",
    "    depthQueue = device.getOutputQueue(name=\"depth\", maxSize=4, blocking=False)\n",
    "    spatialCalcQueue = device.getOutputQueue(name=\"spatialData\", maxSize=4, blocking=False)\n",
    "    spatialCalcConfigInQueue = device.getInputQueue(\"spatialCalcConfig\")\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        x,y,xmin, ymin, xmax, ymax,w,h = centroid_dict[i]\n",
    "        x1 =int(x)-2\n",
    "        y1 = int(y)-2\n",
    "        x2=int(x) +2\n",
    "        y2=int(y) +2\n",
    "        \n",
    "        topLeft = dai.Point2f(x1, y1)\n",
    "        bottomRight = dai.Point2f(x2, y2)\n",
    "        config.roi = dai.Rect(topLeft, bottomRight)\n",
    "        cfg = dai.SpatialLocationCalculatorConfig()\n",
    "        cfg.addROI(config)\n",
    "        spatialCalcConfigInQueue.send(cfg)\n",
    "        # Output queue will be used to get the depth frames from the outputs defined above\n",
    "\n",
    "\n",
    "        color = (255, 255, 255)\n",
    "\n",
    "        #inDepth = depthQueue.get() # Blocking call, will wait until a new data has arrived\n",
    "        #depthFrame = inDepth.getFrame()\n",
    "        #depthFrameColor = cv2.normalize(depthFrame, None, 255, 0, cv2.NORM_INF, cv2.CV_8UC1)\n",
    "        #depthFrameColor = cv2.equalizeHist(depthFrameColor)\n",
    "        #depthFrameColor = cv2.applyColorMap(depthFrameColor, cv2.COLORMAP_HOT)\n",
    "\n",
    "        spatialData = spatialCalcQueue.get().getSpatialLocations()\n",
    "        n = 0\n",
    "        xavg = 0\n",
    "        yavg = 0\n",
    "        zavg = 0\n",
    "        for depthData in spatialData:\n",
    "            #roi = depthData.config.roi\n",
    "            #roi = roi.denormalize(width=depthFrameColor.shape[1], height=depthFrameColor.shape[0])\n",
    "            #xmin = int(roi.topLeft().x)\n",
    "            #ymin = int(roi.topLeft().y)\n",
    "            #xmax = int(roi.bottomRight().x)\n",
    "            #ymax = int(roi.bottomRight().y)\n",
    "\n",
    "            depthMin = depthData.depthMin\n",
    "            depthMax = depthData.depthMax\n",
    "\n",
    "            '''fontType = cv2.FONT_HERSHEY_TRIPLEX\n",
    "            cv2.rectangle(depthFrameColor, (xmin, ymin), (xmax, ymax), color, cv2.FONT_HERSHEY_SCRIPT_SIMPLEX)\n",
    "            cv2.putText(depthFrameColor, f\"X: {int(depthData.spatialCoordinates.x)} mm\", (xmin + 10, ymin + 20), fontType, 0.5, 255)\n",
    "            cv2.putText(depthFrameColor, f\"Y: {int(depthData.spatialCoordinates.y)} mm\", (xmin + 10, ymin + 35), fontType, 0.5, 255)\n",
    "            cv2.putText(depthFrameColor, f\"Z: {int(depthData.spatialCoordinates.z)} mm\", (xmin + 10, ymin + 50), fontType, 0.5, 255)'''\n",
    "            xavg += int(depthData.spatialCoordinates.x)\n",
    "            yavg += int(depthData.spatialCoordinates.y)\n",
    "            zavg += int(depthData.spatialCoordinates.z)\n",
    "            n += 1\n",
    "            \n",
    "        centroid_dict[i] = (int(x), int(y), xmin, ymin, xmax, ymax, xavg/n, yavg/n, zavg/n)\n",
    "        print(i, int(x), int(y), int(w),int(h),int(xavg/n), int(yavg/n), int(zavg/n))\n",
    "        '''img = jetson.utils.cudaFromNumpy(depthFrameColor)\n",
    "        image_widget = ipywidgets.Image(format = 'jpeg')\n",
    "\n",
    "        array = jetson.utils.cudaToNumpy(img)\n",
    "\n",
    "\n",
    "            #print(depthData.spatialCoordinates.x,depthData.spatialCoordinates.y,depthData.spatialCoordinates.z)\n",
    "        image_widget.value = col(array)\n",
    "        display(image_widget)'''\n",
    "    \n",
    "    return(centroid_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "350ca501-0641-4b72-a58b-161cc1a49aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvDrawBoxes(detections, img,distthresh):\n",
    "    distance = 0\n",
    "    \"\"\"\n",
    "    :param:\n",
    "    detections = total detections in one frame\n",
    "    img = image from detect_image method of darknet\n",
    "    :return:\n",
    "    img with bbox\n",
    "    \"\"\"\n",
    "    #================================================================\n",
    "    # 3.1 Purpose : Filter out Persons class from detections and get \n",
    "    #           bounding box centroid for each person detection.\n",
    "    #================================================================\n",
    "    if len(detections) > 0:  \t\t\t\t\t\t# At least 1 detection in the image and check detection presence in a frame  \n",
    "        centroid_dict = dict() \t\t\t\t\t\t# Function creates a dictionary and calls it centroid_dict\n",
    "        objectId = 0\t\t\t\t\t\t\t\t# We inialize a variable called ObjectId and set it to 0\n",
    "        for detection in detections:\t\t\t\t# In this if statement, we filter all the detections for persons only\n",
    "            # Check for the only person name tag \n",
    "            name_tag = detection.ClassID\n",
    "            if name_tag == 1:                \n",
    "                x, y, w, h = detection.Center[0],\\\n",
    "                            detection.Center[1],\\\n",
    "                            detection.Width,\\\n",
    "                            detection.Height      \t# Store the center points of the detections\n",
    "                \n",
    "                xmin, ymin, xmax, ymax = convertBack(float(x), float(y), float(w), float(h))   # Convert from center coordinates to rectangular coordinates, We use floats to ensure the precision of the BBox\n",
    "                \n",
    "                #deptx,depty,deptz = getdepth(objectId,int(x)-2, int(y)-2,int(x) +2, int(y) +2)\n",
    "                #deptx,depty,deptz = getdepth(xmin, ymin, xmax, ymax)\n",
    "                \n",
    "                # Append center point of bbox for persons detected.\n",
    "                centroid_dict[objectId] = (int(x), int(y), xmin, ymin, xmax, ymax,w,h) # Create dictionary of tuple with 'objectId' as the index center points and bbox\n",
    "                objectId += 1 #Increment the index for each detection    \n",
    "                #'''\n",
    "        centroid_dict = get_depth(centroid_dict, objectId)\n",
    "    #=================================================================#\n",
    "    \n",
    "    #=================================================================\n",
    "    # 3.2 Purpose : Determine which person bbox are close to each other\n",
    "    #=================================================================            \t\n",
    "        red_zone_list = [] # List containing which Object id is in under threshold distance condition. \n",
    "        red_line_list = []\n",
    "        for (id1, p1), (id2, p2) in combinations(centroid_dict.items(), 2): # Get all the combinations of close detections, #List of multiple items - id1 1, points 2, 1,3\n",
    "            dx, dy ,dz = p1[6] - p2[6], p1[7] - p2[7],p1[8] - p2[8]  \t# Check the difference between centroid x: 0, y :1\n",
    "            distance = is_close(dx, dy,dz) \t\t\t# Calculates the Euclidean distance\n",
    "            #text = \"distance: {} x1 {} x2 {} y1 {} y2 {} z1 {} z2 {}\".format(str(int(distance)),str(p1[6]),str(p2[6]), str(p1[7]),str(p2[7]),str(p1[8]), str(p2[8]))\n",
    "            text = \"distance: {} x {} y {} z {} \".format(str(int(distance)),str(abs(p1[6] - p2[6])),str(abs( p1[7] - p2[7])), str(abs(p1[8] - p2[8])))\n",
    "            #print('distance: ', str(int(distance)),str(p1[6]),str(p2[6]), str(p1[7]),str(p2[7]),str(p1[8]), str(p2[8]), str(abs(p1[6] - p2[6])),str(abs( p1[7] - p2[7])), str(abs(p1[8] - p2[8])))\n",
    "            location = (10,25)\t\t\t\t\t\t\t\t\t\t\t\t# Set the location of the displayed text\n",
    "            cv2.putText(img, text, location, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA)  # Display Text\n",
    "            if distance <= distthresh:\t\t\t\t\t\t# Set our social distance threshold - If they meet this condition then..\n",
    "                if id1 not in red_zone_list:\n",
    "                    red_zone_list.append(id1)       #  Add Id to a list\n",
    "                    red_line_list.append(p1[0:2])   #  Add points to the list\n",
    "                if id2 not in red_zone_list:\n",
    "                    red_zone_list.append(id2)\t\t# Same for the second id \n",
    "                    red_line_list.append(p2[0:2])\n",
    "        \n",
    "        for idx, box in centroid_dict.items():  # dict (1(key):red(value), 2 blue)  idx - key  box - value\n",
    "            if idx in red_zone_list:   # if id is in red zone list\n",
    "                cv2.rectangle(img, (box[2], box[3]), (box[4], box[5]), (255, 0, 0), 2) # Create Red bounding boxes  #starting point, ending point size of 2\n",
    "            else:\n",
    "                cv2.rectangle(img, (box[2], box[3]), (box[4], box[5]), (0, 255, 0), 2) # Create Green bounding boxes\n",
    "\t\t#=================================================================#\n",
    "\n",
    "\t\t#=================================================================\n",
    "    \t# 3.3 Purpose : Display Risk Analytics and Show Risk Indicators\n",
    "    \t#=================================================================        \n",
    "        #text = \"People at Risk: %s\" % str(len(red_zone_list)) \t\t\t# Count People at Risk\n",
    "        #'''\n",
    "        '''\n",
    "        if distance != 0:\n",
    "            text = \"distance: {} x1 {} x2 {} y1 {} y2 {} z1 {} z2 {}\".format(str(int(distance)),str(p1[6]),str(p2[6]), str(p1[7]),str(p2[7]),str(p1[8]), str(p2[8]))\n",
    "            location = (10,25)\t\t\t\t\t\t\t\t\t\t\t\t# Set the location of the displayed text\n",
    "            cv2.putText(img, text, location, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA)  # Display Text'''\n",
    "\n",
    "        for check in range(0, len(red_line_list)-1):\t\t\t\t\t# Draw line between nearby bboxes iterate through redlist items\n",
    "            start_point = red_line_list[check] \n",
    "            end_point = red_line_list[check+1]\n",
    "            check_line_x = abs(end_point[0] - start_point[0])   \t\t# Calculate the line coordinates for x  \n",
    "            check_line_y = abs(end_point[1] - start_point[1])\t\t\t# Calculate the line coordinates for y\n",
    "            #if (check_line_x < distthresh) and (check_line_y < 25):\t\t\t\t# If both are We check that the lines are below our threshold distance.\n",
    "            cv2.line(img, start_point, end_point, (255, 0, 0), 2)   # Only above the threshold lines are displayed. \n",
    "        #=================================================================#\n",
    "        #'''\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f40004a-34b9-4b9a-a103-e0d388bc2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected cameras:  [<CameraBoardSocket.RGB: 0>, <CameraBoardSocket.LEFT: 1>, <CameraBoardSocket.RIGHT: 2>]\n",
      "Usb speed:  SUPER\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026a1bf6f0b042f99beff84f4ea27cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import depthai as dai\n",
    "\n",
    "# Create pipeline\n",
    "#pipeline = dai.Pipeline()\n",
    "\n",
    "# Define source and output\n",
    "'''camRgb = pipeline.createColorCamera()\n",
    "xoutRgb = pipeline.createXLinkOut()\n",
    "\n",
    "xoutRgb.setStreamName(\"rgb\")\n",
    "\n",
    "# Properties\n",
    "camRgb.setPreviewSize(1280, 720)\n",
    "camRgb.setInterleaved(False)\n",
    "camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.RGB)\n",
    "\n",
    "# Linking\n",
    "camRgb.preview.link(xoutRgb.input)\n",
    "\n",
    "# Connect to device and start pipeline\n",
    "#with dai.Device(pipeline) as device:'''\n",
    "\n",
    "\n",
    "\n",
    "print('Connected cameras: ', device.getConnectedCameras())\n",
    "# Print out usb speed\n",
    "print('Usb speed: ', device.getUsbSpeed().name)\n",
    "\n",
    "# Output queue will be used to get the rgb frames from the output defined above\n",
    "qRgb = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "\n",
    "\n",
    "inRgb = qRgb.get()  # blocking call, will wait until a new data has arrived\n",
    "\n",
    " # Retrieve 'bgr' (opencv format) frame\n",
    "#cv2.imshow(\"rgb\", inRgb.getCvFrame())\n",
    "pic = inRgb.getCvFrame()\n",
    "img = jetson.utils.cudaFromNumpy(pic)\n",
    "image_widget = ipywidgets.Image(format = 'jpeg')\n",
    "        \n",
    "\n",
    "array = jetson.utils.cudaToNumpy(img)\n",
    "    \n",
    "image_widget.value = col(array)\n",
    "display(image_widget)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c06c37-9771-429b-b101-eed7356a0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = jetson.inference.detectNet(\"ssd-mobilenet-v2\", threshold=0.5)\n",
    "\n",
    "#camera = jetson.utils.videoSource(\"/dev/video0\")      # '/dev/video0' for V4L2\n",
    "dis = jetson.utils.videoOutput(\"display://0\") # 'my_video.mp4' for file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d005730-f250-4bc5-8390-8998b9590a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 798 412 914 609 0 0 0\n",
      "0 799 405 914 627 0 0 0\n",
      "0 797 408 919 621 0 0 0\n",
      "0 799 408 914 620 0 0 0\n",
      "0 803 408 915 620 0 0 0\n",
      "0 799 408 920 620 0 0 0\n",
      "0 800 409 918 619 0 0 0\n",
      "0 800 408 912 620 0 0 0\n",
      "0 763 410 985 616 0 0 0\n",
      "0 767 407 993 622 0 0 1838\n",
      "0 776 404 979 629 0 0 1843\n",
      "0 782 405 966 627 0 0 1872\n",
      "0 801 410 929 615 0 0 1873\n",
      "0 673 388 846 645 0 0 1859\n",
      "0 685 384 816 667 0 0 1893\n",
      "0 675 382 831 663 0 0 1896\n",
      "0 688 375 831 673 0 0 1889\n"
     ]
    }
   ],
   "source": [
    "while dis.IsStreaming():\n",
    "    inRgb = qRgb.get()  # blocking call, will wait until a new data has arrived\n",
    "\n",
    "    # Retrieve 'bgr' (opencv format) frame\n",
    "    #cv2.imshow(\"rgb\", inRgb.getCvFrame())\n",
    "    pic = inRgb.getCvFrame()\n",
    "    img = jetson.utils.cudaFromNumpy(pic)\n",
    "    \n",
    "    detections = net.Detect(img)\n",
    "\n",
    "    array = jetson.utils.cudaToNumpy(img)\n",
    "    cvDrawBoxes(detections, array,300)\n",
    "    image_widget.value = col(array)\n",
    "    #display.SetStatus(\"Object Detection | Network {:.0f} FPS\".format(net.GetNetworkFPS()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72e0fbfb-f4bd-4634-bac6-a169fb0a593d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4627c8121aa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjetson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudaFromNumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjetson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudaToNumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mimage_widget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jetcam-0.0.0-py3.6.egg/jetcam/utils.py\u001b[0m in \u001b[0;36mbgr8_to_jpeg\u001b[0;34m(value, quality)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbgr8_to_jpeg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while dis.IsStreaming():\n",
    "    inRgb = qRgb.get()  # blocking call, will wait until a new data has arrived\n",
    "\n",
    "    # Retrieve 'bgr' (opencv format) frame\n",
    "    #cv2.imshow(\"rgb\", inRgb.getCvFrame())\n",
    "    pic = inRgb.getCvFrame()\n",
    "    img = jetson.utils.cudaFromNumpy(pic)\n",
    "    array = jetson.utils.cudaToNumpy(img)\n",
    "    image_widget.value = col(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa06c16-e90d-4d7e-8097-47288cb91090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
